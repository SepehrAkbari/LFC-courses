{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Customer Churn Prediction**\n",
    "\n",
    "**Table of Content:**\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)\n",
    "- [Data Encoding & Imputation](#data-encoding--imputation)\n",
    "- [Handling Imbalanced Data](#handling-imbalanced-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Importing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data.xlsx', sheet_name='E Comm')\n",
    "desc = pd.read_excel('data.xlsx', sheet_name='Data Dict', header=1, usecols=[1,2,3]).drop(columns=\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Description of the data columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Peaking the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Analyzing the type of data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like all the data are in appropriate format, no need to change the data type.\n",
    "\n",
    "2. **Dropping `customerID` column**\n",
    "\n",
    "We'll drop the `customerID` column as it is not useful for our analysis, nor is a feature for churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"CustomerID\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Lets get the count of some interesting variables, to get an idea of what the data represents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,40))\n",
    "n = 1\n",
    "colors = sns.color_palette(\"plasma\", len(df.columns))\n",
    "for i, col in enumerate(df.columns):\n",
    "    plt.subplot(10,2,n)\n",
    "    sns.histplot(df, x=col, bins=25, color=colors[i])\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.grid(True)\n",
    "    n += 1\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a couple of important things these visualizations tell us:\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding & Imputation\n",
    "\n",
    "1. **Checking for missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Count of missing values in each column:\\n\\n{df.isnull().sum()}\\n---')\n",
    "print(f'Total missing: {df.isnull().sum().sum()}')\n",
    "print(f'Total rows missing (at least one): {df[df.isnull().any(axis=1)].shape[0]}')\n",
    "print(f'Percentage of missing values in db: {df.isnull().sum().sum() / df.shape[0] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we cant drop.\n",
    "\n",
    "2. **Imputing missing values for numeric columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "df[numerics] = SimpleImputer(strategy='mean').fit_transform(df[numerics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Encoding categorical columns (One-Hot Encoding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = df.select_dtypes(include=['object']).columns.tolist()\n",
    "df = pd.get_dummies(df, columns=categoricals, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Imputing missing values for categorical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=250), max_iter=10)\n",
    "df = pd.DataFrame(rf_imputer.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Displaying the imputed and encoded data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_corr = df.corr()[\"Churn\"].sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "## plotting the correlation heatmap\n",
    "sns.heatmap(df.corr(), cmap='coolwarm', annot=False)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Specifying the target variable (`y`) and the features (`X`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Churn\"])\n",
    "y = df[\"Churn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Splitting the data into training and testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=250, stratify=y)\n",
    "\n",
    "print(f\"Training set's shape: {X_train.shape}\")\n",
    "print(f\"Testing set's shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Evaluating the imbalance of `Churn`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "churn_percentages = df[\"Churn\"].value_counts(normalize=True) * 100\n",
    "ax = sns.barplot(x=churn_percentages.index, y=churn_percentages.values, palette=\"plasma\")\n",
    "\n",
    "plt.xlabel(\"Churn (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.title(\"Churn Distribution (Scaled)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Handling the imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=250)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before upsampling:\")\n",
    "print('count of Churn = 0: {}'.format(sum(y_train==0)))\n",
    "print('count of Churn = 1: {}'.format(sum(y_train==1)))\n",
    "print(f\"Training set's shape: {X_train.shape}\\n\")\n",
    "\n",
    "print('After upsampling')\n",
    "print('count of Churn = 0: {}'.format(sum(y_train_resampled==0)))\n",
    "print('count of Churn = 1: {}'.format(sum(y_train_resampled==1)))\n",
    "print(f\"Training set's shape: {X_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qs\n",
    "\n",
    "1. is reducing comlumns OK when doing randomForest and boosting?\n",
    "2. confusion matrix before or after SMOT and imputation?\n",
    "3. can i present models more than regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
